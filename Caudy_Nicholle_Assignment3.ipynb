{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 87s 2ms/step - loss: 1.7779 - accuracy: 0.3754 - val_loss: 1.4336 - val_accuracy: 0.4847\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 1.3919 - accuracy: 0.5055 - val_loss: 1.2903 - val_accuracy: 0.5474\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 1.2613 - accuracy: 0.5515 - val_loss: 1.1681 - val_accuracy: 0.5957\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 84s 2ms/step - loss: 1.1753 - accuracy: 0.5846 - val_loss: 1.2074 - val_accuracy: 0.5743\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 1.1045 - accuracy: 0.6144 - val_loss: 1.1458 - val_accuracy: 0.6045\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 79s 2ms/step - loss: 1.0532 - accuracy: 0.6304 - val_loss: 1.0627 - val_accuracy: 0.6339\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.9956 - accuracy: 0.6507 - val_loss: 1.0261 - val_accuracy: 0.6451\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 83s 2ms/step - loss: 0.9535 - accuracy: 0.6647 - val_loss: 1.0416 - val_accuracy: 0.6434\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 77s 2ms/step - loss: 0.9185 - accuracy: 0.6806 - val_loss: 1.0382 - val_accuracy: 0.6535\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 0.8762 - accuracy: 0.6959 - val_loss: 1.0504 - val_accuracy: 0.6441\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.8452 - accuracy: 0.7049 - val_loss: 0.9706 - val_accuracy: 0.6700\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 85s 2ms/step - loss: 0.8163 - accuracy: 0.7168 - val_loss: 0.9867 - val_accuracy: 0.6645\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 0.7843 - accuracy: 0.7293 - val_loss: 1.0277 - val_accuracy: 0.6597\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 79s 2ms/step - loss: 0.7595 - accuracy: 0.7365 - val_loss: 1.0013 - val_accuracy: 0.6629\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.7366 - accuracy: 0.7434 - val_loss: 1.0337 - val_accuracy: 0.6659\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.7130 - accuracy: 0.7545 - val_loss: 0.9952 - val_accuracy: 0.6690\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 77s 2ms/step - loss: 0.6900 - accuracy: 0.7604 - val_loss: 1.0570 - val_accuracy: 0.6719\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 0.6670 - accuracy: 0.7683 - val_loss: 1.1102 - val_accuracy: 0.6376\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 84s 2ms/step - loss: 0.6568 - accuracy: 0.7712 - val_loss: 0.9831 - val_accuracy: 0.6820\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 83s 2ms/step - loss: 0.6372 - accuracy: 0.7825 - val_loss: 1.0877 - val_accuracy: 0.6697\n",
      "10000/10000 [==============================] - 8s 847us/step\n",
      "Test score: 1.086658924293518\n",
      "Test accuracy: 0.6692000031471252\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels \n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /=255\n",
    "X_test /=255\n",
    "\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D (32, (3, 3), padding = 'same',\n",
    "                 input_shape = (IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense (512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIM,\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size = BATCH_SIZE, \n",
    "         epochs = NB_EPOCH, validation_split = VALIDATION_SPLIT, \n",
    "         verbose = VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, \n",
    "                      batch_size = BATCH_SIZE, verbose = VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score [0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# save model\n",
    "model_json = model.to_json()\n",
    "open('cifer10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('cifar10_weights.h5', overwrite = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Augmenting training set images...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels \n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "NUM_TO_AUGMENT = 5\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#augmenting\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(rotation_range = 40, \n",
    "                            width_shift_range = 0.2, \n",
    "                            height_shift_range = 0.2,\n",
    "                            zoom_range = 0.2,\n",
    "                            horizontal_flip = True, \n",
    "                            fill_mode = 'nearest')\n",
    "xtas, ytas = [], []\n",
    "for i in range (X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape ((1,) + x.shape) #(1, 3 , 32, 32)\n",
    "    for x_aug in datagen.flow(x, batch_size=1, \n",
    "                             save_to_dir='preview', \n",
    "                              save_prefix='cifar', \n",
    "                              save_format='jpeg'):\n",
    "        if num_aug >=NUM_TO_AUGMENT:\n",
    "            break\n",
    "            xtax.append(x_aug[0])\n",
    "            num_aug +=1\n",
    "\n",
    "\n",
    "#convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /=255\n",
    "X_test /=255\n",
    "\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D (32, (3, 3), padding = 'same',\n",
    "                 input_shape = (IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense (512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#fit the dataget\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# train\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIM,\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size = BATCH_SIZE, \n",
    "         epochs = NB_EPOCH, validation_split = VALIDATION_SPLIT, \n",
    "         verbose = VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, \n",
    "                      batch_size = BATCH_SIZE, verbose = VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score [0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# save model\n",
    "model_json = model.to_json()\n",
    "open('cifer10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "# save weights\n",
    "model.save_weights('cifar10_weights.h5', overwrite = True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical and Privacy Concerns for Image Recognition Algorithms\n",
    "\n",
    "Ethical and privacy concerns for image recognition algorithms are an ongoing debate in technology and societal roles. A facial recognition app has already been created and in use by law enforcement. Clearview AI was created and enables the user to take a picture of an individual and upload it to see public photos of that individual. These photos can include old pictures that have been scraped from social media without the individual's knowledge. This creates a privacy concern. Even if that individual has a private profile if a friend (or anyone) has shared a photo or video of them on their social media that photo can then be used without the individual's knowledge, and in some cases provide their names. This technology was then sold to different law enforcement agencies to use. A cause for another privacy and ethical issue with this app is that every photo that is taken and uploaded to the Clearview AI app is saved to this company's servers, giving the company access to persons of interest, victims, suspects, and much more. If this data is not secured properly, this information could be used against the individuals in malicious ways. There is also a concern with facial rocognition algorithms in that they seem to struggle with persons of color and accurately identifying their different characteristics. This could cause an individual to be misidentified. Larger cities are now using surveillance with the capability of facial recognition and real-time tracking. Individuals are being recorded and monitored without consent further violating their privacy. This technology could be used to targer marginalized/minority groups if misused. Image recognition can also be used to identify well-known locations and can then be used to identify where a specific person may be, further violating an individual's privacy or potential putting them at risk. I believe these concerns will continue to grow as advancements are made in technology without laws and protections in place. \n",
    "\n",
    "Reference:\n",
    "Hill, K. (2020, January 21). The Secretive Company That Might End Privacy as We Know it. \n",
    "International New York Times.\n",
    "https://link.gale.com/apps/doc/A611773186/GIC?u=nhc_main&sid=ebsco&xid=602284b3\n",
    "\n",
    "Ethical considerations in AI & Maching Learning. Intelegain. (2025, March 20).\n",
    "https://www.intelegain.com/ethical-considerations-in-ai-machine-learning/\n",
    "\n",
    "Almeida, D., Shmarko, K., & Lomas, E. (2022), The ethics of Facial Recognition Technologies, surveillance, and accountability in an age of Artificial Intelligence: A comparative analysis of US, EU, and UK Regulatory frameworks. AI and ethics.\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC8320316/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
